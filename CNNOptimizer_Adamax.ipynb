{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Merve\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 124 samples, validate on 54 samples\n",
      "Epoch 1/25\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 1.0915 - accuracy: 0.3790 - val_loss: 1.0657 - val_accuracy: 0.4074\n",
      "Epoch 2/25\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0525 - accuracy: 0.4032 - val_loss: 1.0305 - val_accuracy: 0.4815\n",
      "Epoch 3/25\n",
      "124/124 [==============================] - 0s 369us/step - loss: 0.9984 - accuracy: 0.5081 - val_loss: 0.9594 - val_accuracy: 0.5926\n",
      "Epoch 4/25\n",
      "124/124 [==============================] - 0s 371us/step - loss: 0.9053 - accuracy: 0.6774 - val_loss: 0.8489 - val_accuracy: 0.7037\n",
      "Epoch 5/25\n",
      "124/124 [==============================] - 0s 395us/step - loss: 0.7841 - accuracy: 0.8226 - val_loss: 0.7255 - val_accuracy: 0.7963\n",
      "Epoch 6/25\n",
      "124/124 [==============================] - 0s 339us/step - loss: 0.6346 - accuracy: 0.9032 - val_loss: 0.5845 - val_accuracy: 0.9074\n",
      "Epoch 7/25\n",
      "124/124 [==============================] - 0s 317us/step - loss: 0.4866 - accuracy: 0.9516 - val_loss: 0.4764 - val_accuracy: 0.8519\n",
      "Epoch 8/25\n",
      "124/124 [==============================] - 0s 339us/step - loss: 0.3397 - accuracy: 0.9758 - val_loss: 0.3661 - val_accuracy: 0.8889\n",
      "Epoch 9/25\n",
      "124/124 [==============================] - 0s 330us/step - loss: 0.2367 - accuracy: 0.9758 - val_loss: 0.3122 - val_accuracy: 0.8889\n",
      "Epoch 10/25\n",
      "124/124 [==============================] - 0s 330us/step - loss: 0.1720 - accuracy: 0.9677 - val_loss: 0.2949 - val_accuracy: 0.8889\n",
      "Epoch 11/25\n",
      "124/124 [==============================] - 0s 314us/step - loss: 0.1309 - accuracy: 0.9677 - val_loss: 0.2735 - val_accuracy: 0.8889\n",
      "Epoch 12/25\n",
      "124/124 [==============================] - 0s 339us/step - loss: 0.1042 - accuracy: 0.9677 - val_loss: 0.2610 - val_accuracy: 0.8889\n",
      "Epoch 13/25\n",
      "124/124 [==============================] - 0s 330us/step - loss: 0.0870 - accuracy: 0.9758 - val_loss: 0.2859 - val_accuracy: 0.8889\n",
      "Epoch 14/25\n",
      "124/124 [==============================] - 0s 363us/step - loss: 0.0757 - accuracy: 0.9839 - val_loss: 0.2791 - val_accuracy: 0.8889\n",
      "Epoch 15/25\n",
      "124/124 [==============================] - 0s 330us/step - loss: 0.0720 - accuracy: 0.9758 - val_loss: 0.2590 - val_accuracy: 0.8889\n",
      "Epoch 16/25\n",
      "124/124 [==============================] - 0s 457us/step - loss: 0.0574 - accuracy: 0.9919 - val_loss: 0.2557 - val_accuracy: 0.8889\n",
      "Epoch 17/25\n",
      "124/124 [==============================] - 0s 459us/step - loss: 0.0503 - accuracy: 0.9919 - val_loss: 0.3004 - val_accuracy: 0.8889\n",
      "Epoch 18/25\n",
      "124/124 [==============================] - 0s 468us/step - loss: 0.0514 - accuracy: 0.9919 - val_loss: 0.2542 - val_accuracy: 0.8889\n",
      "Epoch 19/25\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 1.00 - 0s 395us/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.2565 - val_accuracy: 0.8889\n",
      "Epoch 20/25\n",
      "124/124 [==============================] - 0s 355us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.8889\n",
      "Epoch 21/25\n",
      "124/124 [==============================] - 0s 339us/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.8889\n",
      "Epoch 22/25\n",
      "124/124 [==============================] - 0s 322us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.8889\n",
      "Epoch 23/25\n",
      "124/124 [==============================] - 0s 355us/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.2509 - val_accuracy: 0.8889\n",
      "Epoch 24/25\n",
      "124/124 [==============================] - 0s 379us/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.8889\n",
      "Epoch 25/25\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.8889\n",
      "Test loss: 0.27855821422956606\n",
      "Test accuracy: 0.8888888955116272\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('wine.data',header = None)\n",
    "\n",
    "\n",
    "df.columns = [  'class labels'\n",
    "             \t,'alcohol'\n",
    "             \t,'malicAcid'\n",
    "             \t,'ash'\n",
    "            \t,'ashalcalinity'\n",
    "             \t,'magnesium'\n",
    "            \t,'totalPhenols'\n",
    "             \t,'flavanoids'\n",
    "             \t,'nonFlavanoidPhenols'\n",
    "             \t,'proanthocyanins'\n",
    "            \t,'colorIntensity'\n",
    "             \t,'hue'\n",
    "             \t,'od280_od315'\n",
    "             \t,'proline'\n",
    "                 \n",
    "                ]\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D,Embedding\n",
    "model = Sequential()     \n",
    "model.add(Embedding(1700, 32, input_length=13))\n",
    "model.add(Conv1D(32, 3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training_model=model.fit(X_train, y_train,\n",
    "          batch_size= 15, \n",
    "          epochs=25,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(training_model.history['accuracy'])\n",
    "plt.plot(training_model.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(training_model.history['loss'])\n",
    "plt.plot(training_model.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124 samples, validate on 54 samples\n",
      "Epoch 1/50\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 1.0847 - accuracy: 0.4435 - val_loss: 1.0545 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "124/124 [==============================] - 0s 427us/step - loss: 1.0273 - accuracy: 0.4919 - val_loss: 0.9874 - val_accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "124/124 [==============================] - 0s 363us/step - loss: 0.9417 - accuracy: 0.6371 - val_loss: 0.8829 - val_accuracy: 0.7407\n",
      "Epoch 4/50\n",
      "124/124 [==============================] - 0s 411us/step - loss: 0.8126 - accuracy: 0.8710 - val_loss: 0.7362 - val_accuracy: 0.9074\n",
      "Epoch 5/50\n",
      "124/124 [==============================] - 0s 380us/step - loss: 0.6411 - accuracy: 0.9194 - val_loss: 0.5664 - val_accuracy: 0.9259\n",
      "Epoch 6/50\n",
      "124/124 [==============================] - 0s 361us/step - loss: 0.4635 - accuracy: 0.9194 - val_loss: 0.3987 - val_accuracy: 0.8889\n",
      "Epoch 7/50\n",
      "124/124 [==============================] - 0s 460us/step - loss: 0.3016 - accuracy: 0.9435 - val_loss: 0.2876 - val_accuracy: 0.9074\n",
      "Epoch 8/50\n",
      "124/124 [==============================] - 0s 484us/step - loss: 0.1974 - accuracy: 0.9516 - val_loss: 0.2391 - val_accuracy: 0.9074\n",
      "Epoch 9/50\n",
      "124/124 [==============================] - 0s 459us/step - loss: 0.1453 - accuracy: 0.9677 - val_loss: 0.2110 - val_accuracy: 0.8889\n",
      "Epoch 10/50\n",
      "124/124 [==============================] - 0s 516us/step - loss: 0.1073 - accuracy: 0.9758 - val_loss: 0.2358 - val_accuracy: 0.9074\n",
      "Epoch 11/50\n",
      "124/124 [==============================] - 0s 427us/step - loss: 0.0878 - accuracy: 0.9758 - val_loss: 0.2141 - val_accuracy: 0.9074\n",
      "Epoch 12/50\n",
      "124/124 [==============================] - 0s 516us/step - loss: 0.0715 - accuracy: 0.9839 - val_loss: 0.2050 - val_accuracy: 0.9074\n",
      "Epoch 13/50\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0676 - accuracy: 0.9919 - val_loss: 0.1910 - val_accuracy: 0.9074\n",
      "Epoch 14/50\n",
      "124/124 [==============================] - 0s 500us/step - loss: 0.0609 - accuracy: 0.9758 - val_loss: 0.2388 - val_accuracy: 0.9074\n",
      "Epoch 15/50\n",
      "124/124 [==============================] - 0s 613us/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.1903 - val_accuracy: 0.9074\n",
      "Epoch 16/50\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0428 - accuracy: 0.9919 - val_loss: 0.1914 - val_accuracy: 0.9074\n",
      "Epoch 17/50\n",
      "124/124 [==============================] - 0s 476us/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9074\n",
      "Epoch 18/50\n",
      "124/124 [==============================] - 0s 451us/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9074\n",
      "Epoch 19/50\n",
      "124/124 [==============================] - 0s 524us/step - loss: 0.0317 - accuracy: 0.9919 - val_loss: 0.1942 - val_accuracy: 0.9074\n",
      "Epoch 20/50\n",
      "124/124 [==============================] - 0s 540us/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.2205 - val_accuracy: 0.9074\n",
      "Epoch 21/50\n",
      "124/124 [==============================] - 0s 524us/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9074\n",
      "Epoch 22/50\n",
      "124/124 [==============================] - 0s 596us/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9074\n",
      "Epoch 23/50\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.2219 - val_accuracy: 0.9074\n",
      "Epoch 24/50\n",
      "124/124 [==============================] - 0s 516us/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.2165 - val_accuracy: 0.9074\n",
      "Epoch 25/50\n",
      "124/124 [==============================] - 0s 661us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.9074\n",
      "Epoch 26/50\n",
      "124/124 [==============================] - 0s 677us/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.2206 - val_accuracy: 0.9074\n",
      "Epoch 27/50\n",
      "124/124 [==============================] - 0s 637us/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9074\n",
      "Epoch 28/50\n",
      "124/124 [==============================] - 0s 798us/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2266 - val_accuracy: 0.9074\n",
      "Epoch 29/50\n",
      "124/124 [==============================] - 0s 637us/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 0.9074\n",
      "Epoch 30/50\n",
      "124/124 [==============================] - 0s 629us/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9074\n",
      "Epoch 31/50\n",
      "124/124 [==============================] - 0s 540us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9074\n",
      "Epoch 32/50\n",
      "124/124 [==============================] - 0s 588us/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2335 - val_accuracy: 0.9074\n",
      "Epoch 33/50\n",
      "124/124 [==============================] - 0s 548us/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9074\n",
      "Epoch 34/50\n",
      "124/124 [==============================] - 0s 524us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9074\n",
      "Epoch 35/50\n",
      "124/124 [==============================] - 0s 532us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9074\n",
      "Epoch 36/50\n",
      "124/124 [==============================] - 0s 467us/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.2486 - val_accuracy: 0.9074\n",
      "Epoch 37/50\n",
      "124/124 [==============================] - 0s 548us/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9074\n",
      "Epoch 38/50\n",
      "124/124 [==============================] - 0s 540us/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9074\n",
      "Epoch 39/50\n",
      "124/124 [==============================] - 0s 484us/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 0.9074\n",
      "Epoch 40/50\n",
      "124/124 [==============================] - 0s 548us/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9074\n",
      "Epoch 41/50\n",
      "124/124 [==============================] - 0s 540us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9074\n",
      "Epoch 42/50\n",
      "124/124 [==============================] - 0s 454us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9074\n",
      "Epoch 43/50\n",
      "124/124 [==============================] - 0s 492us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9074\n",
      "Epoch 44/50\n",
      "124/124 [==============================] - 0s 467us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9074\n",
      "Epoch 45/50\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9074\n",
      "Epoch 46/50\n",
      "124/124 [==============================] - 0s 443us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2672 - val_accuracy: 0.9074\n",
      "Epoch 47/50\n",
      "124/124 [==============================] - 0s 484us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9074\n",
      "Epoch 48/50\n",
      "124/124 [==============================] - 0s 484us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.9074\n",
      "Epoch 49/50\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2597 - val_accuracy: 0.9074\n",
      "Epoch 50/50\n",
      "124/124 [==============================] - 0s 540us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9074\n",
      "Test loss: 0.26283759954902863\n",
      "Test accuracy: 0.9074074029922485\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('wine.data',header = None)\n",
    "\n",
    "\n",
    "df.columns = [  'class labels'\n",
    "             \t,'alcohol'\n",
    "             \t,'malicAcid'\n",
    "             \t,'ash'\n",
    "            \t,'ashalcalinity'\n",
    "             \t,'magnesium'\n",
    "            \t,'totalPhenols'\n",
    "             \t,'flavanoids'\n",
    "             \t,'nonFlavanoidPhenols'\n",
    "             \t,'proanthocyanins'\n",
    "            \t,'colorIntensity'\n",
    "             \t,'hue'\n",
    "             \t,'od280_od315'\n",
    "             \t,'proline'\n",
    "                 \n",
    "                ]\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D,Embedding\n",
    "model = Sequential()     \n",
    "model.add(Embedding(1700, 32, input_length=13))\n",
    "model.add(Conv1D(32, 3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training_model=model.fit(X_train, y_train,\n",
    "          batch_size= 15, \n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124 samples, validate on 54 samples\n",
      "Epoch 1/150\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 1.0865 - accuracy: 0.4597 - val_loss: 1.0632 - val_accuracy: 0.7222\n",
      "Epoch 2/150\n",
      "124/124 [==============================] - 0s 387us/step - loss: 1.0349 - accuracy: 0.8306 - val_loss: 0.9996 - val_accuracy: 0.7037\n",
      "Epoch 3/150\n",
      "124/124 [==============================] - 0s 323us/step - loss: 0.9461 - accuracy: 0.8629 - val_loss: 0.8891 - val_accuracy: 0.8333\n",
      "Epoch 4/150\n",
      "124/124 [==============================] - 0s 290us/step - loss: 0.8166 - accuracy: 0.8790 - val_loss: 0.7512 - val_accuracy: 0.7963\n",
      "Epoch 5/150\n",
      "124/124 [==============================] - 0s 404us/step - loss: 0.6721 - accuracy: 0.8871 - val_loss: 0.6083 - val_accuracy: 0.8889\n",
      "Epoch 6/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 0.5211 - accuracy: 0.8629 - val_loss: 0.4754 - val_accuracy: 0.8889\n",
      "Epoch 7/150\n",
      "124/124 [==============================] - 0s 296us/step - loss: 0.3843 - accuracy: 0.8952 - val_loss: 0.3806 - val_accuracy: 0.8889\n",
      "Epoch 8/150\n",
      "124/124 [==============================] - 0s 386us/step - loss: 0.2727 - accuracy: 0.9194 - val_loss: 0.3202 - val_accuracy: 0.8889\n",
      "Epoch 9/150\n",
      "124/124 [==============================] - 0s 363us/step - loss: 0.1991 - accuracy: 0.9516 - val_loss: 0.2853 - val_accuracy: 0.8889\n",
      "Epoch 10/150\n",
      "124/124 [==============================] - 0s 355us/step - loss: 0.1382 - accuracy: 0.9919 - val_loss: 0.2594 - val_accuracy: 0.8889\n",
      "Epoch 11/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 0.1163 - accuracy: 0.9839 - val_loss: 0.2351 - val_accuracy: 0.9074\n",
      "Epoch 12/150\n",
      "124/124 [==============================] - 0s 348us/step - loss: 0.0922 - accuracy: 0.9758 - val_loss: 0.2232 - val_accuracy: 0.8889\n",
      "Epoch 13/150\n",
      "124/124 [==============================] - 0s 360us/step - loss: 0.0742 - accuracy: 0.9839 - val_loss: 0.2208 - val_accuracy: 0.9074\n",
      "Epoch 14/150\n",
      "124/124 [==============================] - 0s 263us/step - loss: 0.0614 - accuracy: 0.9919 - val_loss: 0.2115 - val_accuracy: 0.9074\n",
      "Epoch 15/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0525 - accuracy: 0.9919 - val_loss: 0.2113 - val_accuracy: 0.9074\n",
      "Epoch 16/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.2242 - val_accuracy: 0.9074\n",
      "Epoch 17/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9074\n",
      "Epoch 18/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.2225 - val_accuracy: 0.9074\n",
      "Epoch 19/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0400 - accuracy: 0.9919 - val_loss: 0.2665 - val_accuracy: 0.9074\n",
      "Epoch 20/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.2159 - val_accuracy: 0.9074\n",
      "Epoch 21/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.2095 - val_accuracy: 0.9074\n",
      "Epoch 22/150\n",
      "124/124 [==============================] - 0s 231us/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9074\n",
      "Epoch 23/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.9074\n",
      "Epoch 24/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 0.9074\n",
      "Epoch 25/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9074\n",
      "Epoch 26/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9074\n",
      "Epoch 27/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9074\n",
      "Epoch 28/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9074\n",
      "Epoch 29/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9074\n",
      "Epoch 30/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 0.9074\n",
      "Epoch 31/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2456 - val_accuracy: 0.9074\n",
      "Epoch 32/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9074\n",
      "Epoch 33/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2604 - val_accuracy: 0.9074\n",
      "Epoch 34/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2558 - val_accuracy: 0.9074\n",
      "Epoch 35/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9074\n",
      "Epoch 36/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9074\n",
      "Epoch 37/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2595 - val_accuracy: 0.9074\n",
      "Epoch 38/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2571 - val_accuracy: 0.9074\n",
      "Epoch 39/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.2605 - val_accuracy: 0.9074\n",
      "Epoch 40/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2613 - val_accuracy: 0.9074\n",
      "Epoch 41/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9074\n",
      "Epoch 42/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2669 - val_accuracy: 0.9074\n",
      "Epoch 43/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 0.9074\n",
      "Epoch 44/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2681 - val_accuracy: 0.9074\n",
      "Epoch 45/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 0.9074\n",
      "Epoch 46/150\n",
      "124/124 [==============================] - 0s 434us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9074\n",
      "Epoch 47/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2792 - val_accuracy: 0.9074\n",
      "Epoch 48/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2782 - val_accuracy: 0.9074\n",
      "Epoch 49/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2754 - val_accuracy: 0.9074\n",
      "Epoch 50/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9074\n",
      "Epoch 51/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9074\n",
      "Epoch 52/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9074\n",
      "Epoch 53/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9074\n",
      "Epoch 54/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9074\n",
      "Epoch 55/150\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2904 - val_accuracy: 0.9074\n",
      "Epoch 56/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 0.9074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "124/124 [==============================] - 0s 486us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2904 - val_accuracy: 0.9074\n",
      "Epoch 58/150\n",
      "124/124 [==============================] - 0s 322us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2911 - val_accuracy: 0.9074\n",
      "Epoch 59/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2904 - val_accuracy: 0.9074\n",
      "Epoch 60/150\n",
      "124/124 [==============================] - 0s 712us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2911 - val_accuracy: 0.9074\n",
      "Epoch 61/150\n",
      "124/124 [==============================] - 0s 395us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2966 - val_accuracy: 0.9074\n",
      "Epoch 62/150\n",
      "124/124 [==============================] - 0s 467us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9074\n",
      "Epoch 63/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9074\n",
      "Epoch 64/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3020 - val_accuracy: 0.9074\n",
      "Epoch 65/150\n",
      "124/124 [==============================] - 0s 403us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.9074\n",
      "Epoch 66/150\n",
      "124/124 [==============================] - 0s 403us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3010 - val_accuracy: 0.9074\n",
      "Epoch 67/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3033 - val_accuracy: 0.9074\n",
      "Epoch 68/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3049 - val_accuracy: 0.9074\n",
      "Epoch 69/150\n",
      "124/124 [==============================] - 0s 387us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3036 - val_accuracy: 0.9074\n",
      "Epoch 70/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9074\n",
      "Epoch 71/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9074\n",
      "Epoch 72/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9074\n",
      "Epoch 73/150\n",
      "124/124 [==============================] - 0s 395us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3092 - val_accuracy: 0.9074\n",
      "Epoch 74/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3112 - val_accuracy: 0.9074\n",
      "Epoch 75/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 0.9074\n",
      "Epoch 76/150\n",
      "124/124 [==============================] - 0s 389us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3153 - val_accuracy: 0.9074\n",
      "Epoch 77/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3162 - val_accuracy: 0.9074\n",
      "Epoch 78/150\n",
      "124/124 [==============================] - 0s 406us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 0.9074\n",
      "Epoch 79/150\n",
      "124/124 [==============================] - 0s 403us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3192 - val_accuracy: 0.9074\n",
      "Epoch 80/150\n",
      "124/124 [==============================] - 0s 379us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 0.9074\n",
      "Epoch 81/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3253 - val_accuracy: 0.9074\n",
      "Epoch 82/150\n",
      "124/124 [==============================] - 0s 508us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.9074\n",
      "Epoch 83/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 0.9074\n",
      "Epoch 84/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 0.9074\n",
      "Epoch 85/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9074\n",
      "Epoch 86/150\n",
      "124/124 [==============================] - 0s 451us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3294 - val_accuracy: 0.9074\n",
      "Epoch 87/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.9074\n",
      "Epoch 88/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 0.9074\n",
      "Epoch 89/150\n",
      "124/124 [==============================] - 0s 403us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3331 - val_accuracy: 0.9074\n",
      "Epoch 90/150\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3321 - val_accuracy: 0.9074\n",
      "Epoch 91/150\n",
      "124/124 [==============================] - 0s 387us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.9074\n",
      "Epoch 92/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 9.8901e-04 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 0.9074\n",
      "Epoch 93/150\n",
      "124/124 [==============================] - 0s 792us/step - loss: 9.7193e-04 - accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9074\n",
      "Epoch 94/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 9.4409e-04 - accuracy: 1.0000 - val_loss: 0.3316 - val_accuracy: 0.9074\n",
      "Epoch 95/150\n",
      "124/124 [==============================] - 0s 451us/step - loss: 9.2479e-04 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9074\n",
      "Epoch 96/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 8.9940e-04 - accuracy: 1.0000 - val_loss: 0.3421 - val_accuracy: 0.9074\n",
      "Epoch 97/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 8.7674e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9074\n",
      "Epoch 98/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 8.6129e-04 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9074\n",
      "Epoch 99/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 8.3377e-04 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9074\n",
      "Epoch 100/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 8.1664e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9074\n",
      "Epoch 101/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 7.8793e-04 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9074\n",
      "Epoch 102/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 7.7269e-04 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9074\n",
      "Epoch 103/150\n",
      "124/124 [==============================] - 0s 476us/step - loss: 7.5569e-04 - accuracy: 1.0000 - val_loss: 0.3439 - val_accuracy: 0.9074\n",
      "Epoch 104/150\n",
      "124/124 [==============================] - 0s 467us/step - loss: 7.3839e-04 - accuracy: 1.0000 - val_loss: 0.3457 - val_accuracy: 0.9074\n",
      "Epoch 105/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 7.2628e-04 - accuracy: 1.0000 - val_loss: 0.3491 - val_accuracy: 0.9074\n",
      "Epoch 106/150\n",
      "124/124 [==============================] - 0s 443us/step - loss: 7.0806e-04 - accuracy: 1.0000 - val_loss: 0.3480 - val_accuracy: 0.9074\n",
      "Epoch 107/150\n",
      "124/124 [==============================] - 0s 669us/step - loss: 6.9465e-04 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9074\n",
      "Epoch 108/150\n",
      "124/124 [==============================] - 0s 814us/step - loss: 6.7591e-04 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9074\n",
      "Epoch 109/150\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7976e-04 - accuracy: 1.00 - 0s 596us/step - loss: 6.6036e-04 - accuracy: 1.0000 - val_loss: 0.3500 - val_accuracy: 0.9074\n",
      "Epoch 110/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 6.4389e-04 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 0.9074\n",
      "Epoch 111/150\n",
      "124/124 [==============================] - 0s 371us/step - loss: 6.3314e-04 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.9074\n",
      "Epoch 112/150\n",
      "124/124 [==============================] - 0s 347us/step - loss: 6.1867e-04 - accuracy: 1.0000 - val_loss: 0.3519 - val_accuracy: 0.9074\n",
      "Epoch 113/150\n",
      "124/124 [==============================] - 0s 443us/step - loss: 6.0388e-04 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9074\n",
      "Epoch 114/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 5.9546e-04 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 0.9074\n",
      "Epoch 115/150\n",
      "124/124 [==============================] - 0s 411us/step - loss: 5.8051e-04 - accuracy: 1.0000 - val_loss: 0.3567 - val_accuracy: 0.9074\n",
      "Epoch 116/150\n",
      "124/124 [==============================] - 0s 371us/step - loss: 5.6799e-04 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.9074\n",
      "Epoch 117/150\n",
      "124/124 [==============================] - 0s 371us/step - loss: 5.5594e-04 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.9074\n",
      "Epoch 118/150\n",
      "124/124 [==============================] - 0s 371us/step - loss: 5.4748e-04 - accuracy: 1.0000 - val_loss: 0.3584 - val_accuracy: 0.9074\n",
      "Epoch 119/150\n",
      "124/124 [==============================] - 0s 395us/step - loss: 5.3674e-04 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.9074\n",
      "Epoch 120/150\n",
      "124/124 [==============================] - 0s 637us/step - loss: 5.2539e-04 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 0.9074\n",
      "Epoch 121/150\n",
      "124/124 [==============================] - 0s 790us/step - loss: 5.1449e-04 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 0.9074\n",
      "Epoch 122/150\n",
      "124/124 [==============================] - 0s 750us/step - loss: 5.0394e-04 - accuracy: 1.0000 - val_loss: 0.3666 - val_accuracy: 0.9074\n",
      "Epoch 123/150\n",
      "124/124 [==============================] - 0s 564us/step - loss: 4.9420e-04 - accuracy: 1.0000 - val_loss: 0.3673 - val_accuracy: 0.9074\n",
      "Epoch 124/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 4.8384e-04 - accuracy: 1.0000 - val_loss: 0.3660 - val_accuracy: 0.9074\n",
      "Epoch 125/150\n",
      "124/124 [==============================] - 0s 500us/step - loss: 4.7182e-04 - accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9074\n",
      "Epoch 126/150\n",
      "124/124 [==============================] - 0s 508us/step - loss: 4.6002e-04 - accuracy: 1.0000 - val_loss: 0.3632 - val_accuracy: 0.9074\n",
      "Epoch 127/150\n",
      "124/124 [==============================] - 0s 540us/step - loss: 4.5116e-04 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 0.9074\n",
      "Epoch 128/150\n",
      "124/124 [==============================] - 0s 459us/step - loss: 4.4525e-04 - accuracy: 1.0000 - val_loss: 0.3605 - val_accuracy: 0.9074\n",
      "Epoch 129/150\n",
      "124/124 [==============================] - 0s 476us/step - loss: 4.3328e-04 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 0.9074\n",
      "Epoch 130/150\n",
      "124/124 [==============================] - 0s 476us/step - loss: 4.2436e-04 - accuracy: 1.0000 - val_loss: 0.3650 - val_accuracy: 0.9074\n",
      "Epoch 131/150\n",
      "124/124 [==============================] - 0s 443us/step - loss: 4.1634e-04 - accuracy: 1.0000 - val_loss: 0.3675 - val_accuracy: 0.9074\n",
      "Epoch 132/150\n",
      "124/124 [==============================] - 0s 484us/step - loss: 4.0683e-04 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9074\n",
      "Epoch 133/150\n",
      "124/124 [==============================] - 0s 476us/step - loss: 3.9840e-04 - accuracy: 1.0000 - val_loss: 0.3684 - val_accuracy: 0.9074\n",
      "Epoch 134/150\n",
      "124/124 [==============================] - 0s 605us/step - loss: 3.8989e-04 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9074\n",
      "Epoch 135/150\n",
      "124/124 [==============================] - 0s 635us/step - loss: 3.8437e-04 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9074\n",
      "Epoch 136/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 3.7494e-04 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 0.9074\n",
      "Epoch 137/150\n",
      "124/124 [==============================] - 0s 451us/step - loss: 3.7016e-04 - accuracy: 1.0000 - val_loss: 0.3744 - val_accuracy: 0.9074\n",
      "Epoch 138/150\n",
      "124/124 [==============================] - 0s 299us/step - loss: 3.6058e-04 - accuracy: 1.0000 - val_loss: 0.3768 - val_accuracy: 0.9074\n",
      "Epoch 139/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.5353e-04 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.9074\n",
      "Epoch 140/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.4685e-04 - accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 0.9074\n",
      "Epoch 141/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.4228e-04 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 0.9074\n",
      "Epoch 142/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.3633e-04 - accuracy: 1.0000 - val_loss: 0.3871 - val_accuracy: 0.9074\n",
      "Epoch 143/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.3063e-04 - accuracy: 1.0000 - val_loss: 0.3841 - val_accuracy: 0.9074\n",
      "Epoch 144/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.2171e-04 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 0.9074\n",
      "Epoch 145/150\n",
      "124/124 [==============================] - 0s 378us/step - loss: 3.1767e-04 - accuracy: 1.0000 - val_loss: 0.3879 - val_accuracy: 0.9074\n",
      "Epoch 146/150\n",
      "124/124 [==============================] - 0s 458us/step - loss: 3.0989e-04 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 0.9074\n",
      "Epoch 147/150\n",
      "124/124 [==============================] - 0s 427us/step - loss: 3.0448e-04 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.9074\n",
      "Epoch 148/150\n",
      "124/124 [==============================] - 0s 371us/step - loss: 2.9786e-04 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 0.9074\n",
      "Epoch 149/150\n",
      "124/124 [==============================] - 0s 435us/step - loss: 2.9166e-04 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.9074\n",
      "Epoch 150/150\n",
      "124/124 [==============================] - 0s 403us/step - loss: 2.8802e-04 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 0.9074\n",
      "Test loss: 0.38528714304858885\n",
      "Test accuracy: 0.9074074029922485\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('wine.data',header = None)\n",
    "\n",
    "\n",
    "df.columns = [  'class labels'\n",
    "             \t,'alcohol'\n",
    "             \t,'malicAcid'\n",
    "             \t,'ash'\n",
    "            \t,'ashalcalinity'\n",
    "             \t,'magnesium'\n",
    "            \t,'totalPhenols'\n",
    "             \t,'flavanoids'\n",
    "             \t,'nonFlavanoidPhenols'\n",
    "             \t,'proanthocyanins'\n",
    "            \t,'colorIntensity'\n",
    "             \t,'hue'\n",
    "             \t,'od280_od315'\n",
    "             \t,'proline'\n",
    "                 \n",
    "                ]\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D,Embedding\n",
    "model = Sequential()     \n",
    "model.add(Embedding(1700, 32, input_length=13))\n",
    "model.add(Conv1D(32, 3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training_model=model.fit(X_train, y_train,\n",
    "          batch_size= 15, \n",
    "          epochs=150,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124 samples, validate on 54 samples\n",
      "Epoch 1/250\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 1.0894 - accuracy: 0.4113 - val_loss: 1.0650 - val_accuracy: 0.6296\n",
      "Epoch 2/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 1.0383 - accuracy: 0.7984 - val_loss: 1.0044 - val_accuracy: 0.7222\n",
      "Epoch 3/250\n",
      "124/124 [==============================] - 0s 406us/step - loss: 0.9589 - accuracy: 0.8468 - val_loss: 0.9092 - val_accuracy: 0.7222\n",
      "Epoch 4/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 0.8412 - accuracy: 0.7903 - val_loss: 0.7681 - val_accuracy: 0.7407\n",
      "Epoch 5/250\n",
      "124/124 [==============================] - 0s 399us/step - loss: 0.6729 - accuracy: 0.8952 - val_loss: 0.6098 - val_accuracy: 0.9074\n",
      "Epoch 6/250\n",
      "124/124 [==============================] - 0s 404us/step - loss: 0.5038 - accuracy: 0.9032 - val_loss: 0.4589 - val_accuracy: 0.9074\n",
      "Epoch 7/250\n",
      "124/124 [==============================] - 0s 661us/step - loss: 0.3617 - accuracy: 0.9113 - val_loss: 0.3553 - val_accuracy: 0.8889\n",
      "Epoch 8/250\n",
      "124/124 [==============================] - 0s 548us/step - loss: 0.2525 - accuracy: 0.9355 - val_loss: 0.3057 - val_accuracy: 0.8889\n",
      "Epoch 9/250\n",
      "124/124 [==============================] - 0s 536us/step - loss: 0.1804 - accuracy: 0.9597 - val_loss: 0.2949 - val_accuracy: 0.8889\n",
      "Epoch 10/250\n",
      "124/124 [==============================] - 0s 669us/step - loss: 0.1500 - accuracy: 0.9516 - val_loss: 0.2849 - val_accuracy: 0.8889\n",
      "Epoch 11/250\n",
      "124/124 [==============================] - 0s 738us/step - loss: 0.1230 - accuracy: 0.9677 - val_loss: 0.2630 - val_accuracy: 0.8889\n",
      "Epoch 12/250\n",
      "124/124 [==============================] - 0s 689us/step - loss: 0.1036 - accuracy: 0.9758 - val_loss: 0.2617 - val_accuracy: 0.8889\n",
      "Epoch 13/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0878 - accuracy: 0.9839 - val_loss: 0.2582 - val_accuracy: 0.8889\n",
      "Epoch 14/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0804 - accuracy: 0.9839 - val_loss: 0.2459 - val_accuracy: 0.8889\n",
      "Epoch 15/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0679 - accuracy: 0.9839 - val_loss: 0.2699 - val_accuracy: 0.8889\n",
      "Epoch 16/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0622 - accuracy: 0.9839 - val_loss: 0.2427 - val_accuracy: 0.8889\n",
      "Epoch 17/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0541 - accuracy: 0.9919 - val_loss: 0.2416 - val_accuracy: 0.8889\n",
      "Epoch 18/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0522 - accuracy: 0.9919 - val_loss: 0.2556 - val_accuracy: 0.8889\n",
      "Epoch 19/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.8889\n",
      "Epoch 20/250\n",
      "124/124 [==============================] - 0s 453us/step - loss: 0.0397 - accuracy: 0.9919 - val_loss: 0.2370 - val_accuracy: 0.8889\n",
      "Epoch 21/250\n",
      "124/124 [==============================] - 0s 500us/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.2566 - val_accuracy: 0.8889\n",
      "Epoch 22/250\n",
      "124/124 [==============================] - 0s 492us/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.2444 - val_accuracy: 0.8889\n",
      "Epoch 23/250\n",
      "124/124 [==============================] - 0s 500us/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.8889\n",
      "Epoch 24/250\n",
      "124/124 [==============================] - 0s 484us/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.2490 - val_accuracy: 0.8889\n",
      "Epoch 25/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.2572 - val_accuracy: 0.8889\n",
      "Epoch 26/250\n",
      "124/124 [==============================] - 0s 386us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.2577 - val_accuracy: 0.8889\n",
      "Epoch 27/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.8889\n",
      "Epoch 28/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.8889\n",
      "Epoch 29/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.8889\n",
      "Epoch 30/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.8889\n",
      "Epoch 31/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.8889\n",
      "Epoch 32/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.8889\n",
      "Epoch 33/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 0.8889\n",
      "Epoch 34/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2527 - val_accuracy: 0.8889\n",
      "Epoch 35/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.8889\n",
      "Epoch 36/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.8889\n",
      "Epoch 37/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 0.8889\n",
      "Epoch 38/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.8889\n",
      "Epoch 39/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.8889\n",
      "Epoch 40/250\n",
      "124/124 [==============================] - 0s 416us/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.8889\n",
      "Epoch 41/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.8889\n",
      "Epoch 42/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2778 - val_accuracy: 0.8889\n",
      "Epoch 43/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.8889\n",
      "Epoch 44/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.8889\n",
      "Epoch 45/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 0.8889\n",
      "Epoch 46/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 0.8889\n",
      "Epoch 47/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.8889\n",
      "Epoch 48/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 0.8889\n",
      "Epoch 49/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2902 - val_accuracy: 0.8889\n",
      "Epoch 50/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2906 - val_accuracy: 0.8889\n",
      "Epoch 51/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.8889\n",
      "Epoch 52/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2942 - val_accuracy: 0.8889\n",
      "Epoch 53/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2927 - val_accuracy: 0.8889\n",
      "Epoch 54/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2925 - val_accuracy: 0.8889\n",
      "Epoch 55/250\n",
      "124/124 [==============================] - 0s 630us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.8889\n",
      "Epoch 56/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3062 - val_accuracy: 0.8889\n",
      "Epoch 58/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.8889\n",
      "Epoch 59/250\n",
      "124/124 [==============================] - 0s 400us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3079 - val_accuracy: 0.8889\n",
      "Epoch 60/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3067 - val_accuracy: 0.8889\n",
      "Epoch 61/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3038 - val_accuracy: 0.8889\n",
      "Epoch 62/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3023 - val_accuracy: 0.8889\n",
      "Epoch 63/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3058 - val_accuracy: 0.8889\n",
      "Epoch 64/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.8889\n",
      "Epoch 65/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3088 - val_accuracy: 0.8889\n",
      "Epoch 66/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3130 - val_accuracy: 0.8889\n",
      "Epoch 67/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.8889\n",
      "Epoch 68/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.8889\n",
      "Epoch 69/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3123 - val_accuracy: 0.8889\n",
      "Epoch 70/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3146 - val_accuracy: 0.8889\n",
      "Epoch 71/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3167 - val_accuracy: 0.8889\n",
      "Epoch 72/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3216 - val_accuracy: 0.8889\n",
      "Epoch 73/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.8889\n",
      "Epoch 74/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.8889\n",
      "Epoch 75/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3229 - val_accuracy: 0.8889\n",
      "Epoch 76/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3237 - val_accuracy: 0.8889\n",
      "Epoch 77/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3238 - val_accuracy: 0.8889\n",
      "Epoch 78/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3245 - val_accuracy: 0.8889\n",
      "Epoch 79/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.8889\n",
      "Epoch 80/250\n",
      "124/124 [==============================] - 0s 473us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3283 - val_accuracy: 0.8889\n",
      "Epoch 81/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3287 - val_accuracy: 0.8889\n",
      "Epoch 82/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.8889\n",
      "Epoch 83/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3309 - val_accuracy: 0.8889\n",
      "Epoch 84/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3316 - val_accuracy: 0.8889\n",
      "Epoch 85/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3326 - val_accuracy: 0.8889\n",
      "Epoch 86/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.8889\n",
      "Epoch 87/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.8889\n",
      "Epoch 88/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3378 - val_accuracy: 0.8889\n",
      "Epoch 89/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3395 - val_accuracy: 0.8889\n",
      "Epoch 90/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.8889\n",
      "Epoch 91/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.8889\n",
      "Epoch 92/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 0.8889\n",
      "Epoch 93/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3423 - val_accuracy: 0.8889\n",
      "Epoch 94/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3444 - val_accuracy: 0.8889\n",
      "Epoch 95/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.8889\n",
      "Epoch 96/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.8889\n",
      "Epoch 97/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3462 - val_accuracy: 0.8889\n",
      "Epoch 98/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 9.8836e-04 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.8889\n",
      "Epoch 99/250\n",
      "124/124 [==============================] - 0s 444us/step - loss: 9.5975e-04 - accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 0.8889\n",
      "Epoch 100/250\n",
      "124/124 [==============================] - 0s 448us/step - loss: 9.3749e-04 - accuracy: 1.0000 - val_loss: 0.3491 - val_accuracy: 0.8889\n",
      "Epoch 101/250\n",
      "124/124 [==============================] - 0s 259us/step - loss: 9.1893e-04 - accuracy: 1.0000 - val_loss: 0.3502 - val_accuracy: 0.8889\n",
      "Epoch 102/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 8.9536e-04 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 0.8889\n",
      "Epoch 103/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 8.7200e-04 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.8889\n",
      "Epoch 104/250\n",
      "124/124 [==============================] - 0s 447us/step - loss: 8.5552e-04 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.8889\n",
      "Epoch 105/250\n",
      "124/124 [==============================] - 0s 772us/step - loss: 8.3952e-04 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 0.8889\n",
      "Epoch 106/250\n",
      "124/124 [==============================] - 0s 414us/step - loss: 8.2339e-04 - accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.8889\n",
      "Epoch 107/250\n",
      "124/124 [==============================] - 0s 411us/step - loss: 8.0344e-04 - accuracy: 1.0000 - val_loss: 0.3537 - val_accuracy: 0.8889\n",
      "Epoch 108/250\n",
      "124/124 [==============================] - 0s 451us/step - loss: 7.8573e-04 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.8889\n",
      "Epoch 109/250\n",
      "124/124 [==============================] - 0s 379us/step - loss: 7.7290e-04 - accuracy: 1.0000 - val_loss: 0.3563 - val_accuracy: 0.8889\n",
      "Epoch 110/250\n",
      "124/124 [==============================] - 0s 349us/step - loss: 7.5549e-04 - accuracy: 1.0000 - val_loss: 0.3563 - val_accuracy: 0.8889\n",
      "Epoch 111/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 7.4233e-04 - accuracy: 1.0000 - val_loss: 0.3558 - val_accuracy: 0.8889\n",
      "Epoch 112/250\n",
      "124/124 [==============================] - 0s 751us/step - loss: 7.2389e-04 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.8889\n",
      "Epoch 113/250\n",
      "124/124 [==============================] - 0s 484us/step - loss: 7.1030e-04 - accuracy: 1.0000 - val_loss: 0.3583 - val_accuracy: 0.8889\n",
      "Epoch 114/250\n",
      "124/124 [==============================] - 0s 588us/step - loss: 6.9487e-04 - accuracy: 1.0000 - val_loss: 0.3596 - val_accuracy: 0.8889\n",
      "Epoch 115/250\n",
      "124/124 [==============================] - 0s 789us/step - loss: 6.8329e-04 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.8889\n",
      "Epoch 116/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 6.6730e-04 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 0.8889\n",
      "Epoch 117/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 6.5324e-04 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.8889\n",
      "Epoch 118/250\n",
      "124/124 [==============================] - 0s 401us/step - loss: 6.4163e-04 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.8889\n",
      "Epoch 119/250\n",
      "124/124 [==============================] - 0s 469us/step - loss: 6.2773e-04 - accuracy: 1.0000 - val_loss: 0.3633 - val_accuracy: 0.8889\n",
      "Epoch 120/250\n",
      "124/124 [==============================] - 0s 450us/step - loss: 6.1740e-04 - accuracy: 1.0000 - val_loss: 0.3664 - val_accuracy: 0.8889\n",
      "Epoch 121/250\n",
      "124/124 [==============================] - 0s 467us/step - loss: 6.0721e-04 - accuracy: 1.0000 - val_loss: 0.3663 - val_accuracy: 0.8889\n",
      "Epoch 122/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 5.9168e-04 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.8889\n",
      "Epoch 123/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 5.8039e-04 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.8889\n",
      "Epoch 124/250\n",
      "124/124 [==============================] - 0s 435us/step - loss: 5.6879e-04 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.8889\n",
      "Epoch 125/250\n",
      "124/124 [==============================] - 0s 427us/step - loss: 5.5660e-04 - accuracy: 1.0000 - val_loss: 0.3706 - val_accuracy: 0.8889\n",
      "Epoch 126/250\n",
      "124/124 [==============================] - 0s 395us/step - loss: 5.4488e-04 - accuracy: 1.0000 - val_loss: 0.3683 - val_accuracy: 0.8889\n",
      "Epoch 127/250\n",
      "124/124 [==============================] - 0s 407us/step - loss: 5.3205e-04 - accuracy: 1.0000 - val_loss: 0.3687 - val_accuracy: 0.9074\n",
      "Epoch 128/250\n",
      "124/124 [==============================] - 0s 435us/step - loss: 5.2204e-04 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9074\n",
      "Epoch 129/250\n",
      "124/124 [==============================] - 0s 392us/step - loss: 5.1147e-04 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9074\n",
      "Epoch 130/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 5.0189e-04 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9074\n",
      "Epoch 131/250\n",
      "124/124 [==============================] - 0s 427us/step - loss: 4.9294e-04 - accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 0.9074\n",
      "Epoch 132/250\n",
      "124/124 [==============================] - 0s 400us/step - loss: 4.8431e-04 - accuracy: 1.0000 - val_loss: 0.3750 - val_accuracy: 0.8889\n",
      "Epoch 133/250\n",
      "124/124 [==============================] - 0s 411us/step - loss: 4.7398e-04 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.8889\n",
      "Epoch 134/250\n",
      "124/124 [==============================] - 0s 435us/step - loss: 4.6623e-04 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.8889\n",
      "Epoch 135/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 4.5703e-04 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.8889\n",
      "Epoch 136/250\n",
      "124/124 [==============================] - 0s 392us/step - loss: 4.5038e-04 - accuracy: 1.0000 - val_loss: 0.3767 - val_accuracy: 0.8889\n",
      "Epoch 137/250\n",
      "124/124 [==============================] - 0s 427us/step - loss: 4.3914e-04 - accuracy: 1.0000 - val_loss: 0.3774 - val_accuracy: 0.8889\n",
      "Epoch 138/250\n",
      "124/124 [==============================] - 0s 375us/step - loss: 4.3325e-04 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.8889\n",
      "Epoch 139/250\n",
      "124/124 [==============================] - 0s 389us/step - loss: 4.2396e-04 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.8889\n",
      "Epoch 140/250\n",
      "124/124 [==============================] - 0s 403us/step - loss: 4.1619e-04 - accuracy: 1.0000 - val_loss: 0.3811 - val_accuracy: 0.8889\n",
      "Epoch 141/250\n",
      "124/124 [==============================] - 0s 435us/step - loss: 4.0942e-04 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 0.8889\n",
      "Epoch 142/250\n",
      "124/124 [==============================] - 0s 383us/step - loss: 4.0220e-04 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9074\n",
      "Epoch 143/250\n",
      "124/124 [==============================] - 0s 451us/step - loss: 3.9437e-04 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9074\n",
      "Epoch 144/250\n",
      "124/124 [==============================] - 0s 419us/step - loss: 3.8798e-04 - accuracy: 1.0000 - val_loss: 0.3836 - val_accuracy: 0.8889\n",
      "Epoch 145/250\n",
      "124/124 [==============================] - 0s 406us/step - loss: 3.8234e-04 - accuracy: 1.0000 - val_loss: 0.3851 - val_accuracy: 0.8889\n",
      "Epoch 146/250\n",
      "124/124 [==============================] - 0s 387us/step - loss: 3.7500e-04 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.8889\n",
      "Epoch 147/250\n",
      "124/124 [==============================] - 0s 427us/step - loss: 3.6821e-04 - accuracy: 1.0000 - val_loss: 0.3877 - val_accuracy: 0.8889\n",
      "Epoch 148/250\n",
      "124/124 [==============================] - 0s 637us/step - loss: 3.6252e-04 - accuracy: 1.0000 - val_loss: 0.3897 - val_accuracy: 0.8889\n",
      "Epoch 149/250\n",
      "124/124 [==============================] - 0s 508us/step - loss: 3.5686e-04 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 0.8889\n",
      "Epoch 150/250\n",
      "124/124 [==============================] - 0s 532us/step - loss: 3.5084e-04 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 0.8889\n",
      "Epoch 151/250\n",
      "124/124 [==============================] - 0s 508us/step - loss: 3.4501e-04 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.8889\n",
      "Epoch 152/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 3.3787e-04 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.8889\n",
      "Epoch 153/250\n",
      "124/124 [==============================] - 0s 476us/step - loss: 3.3212e-04 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 0.8889\n",
      "Epoch 154/250\n",
      "124/124 [==============================] - 0s 562us/step - loss: 3.2636e-04 - accuracy: 1.0000 - val_loss: 0.3933 - val_accuracy: 0.8889\n",
      "Epoch 155/250\n",
      "124/124 [==============================] - 0s 548us/step - loss: 3.2088e-04 - accuracy: 1.0000 - val_loss: 0.3943 - val_accuracy: 0.8889\n",
      "Epoch 156/250\n",
      "124/124 [==============================] - 0s 467us/step - loss: 3.1623e-04 - accuracy: 1.0000 - val_loss: 0.3936 - val_accuracy: 0.9074\n",
      "Epoch 157/250\n",
      "124/124 [==============================] - 0s 508us/step - loss: 3.1104e-04 - accuracy: 1.0000 - val_loss: 0.3952 - val_accuracy: 0.8889\n",
      "Epoch 158/250\n",
      "124/124 [==============================] - 0s 759us/step - loss: 3.0554e-04 - accuracy: 1.0000 - val_loss: 0.3959 - val_accuracy: 0.8889\n",
      "Epoch 159/250\n",
      "124/124 [==============================] - 0s 654us/step - loss: 3.0020e-04 - accuracy: 1.0000 - val_loss: 0.3965 - val_accuracy: 0.8889\n",
      "Epoch 160/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.9466e-04 - accuracy: 1.0000 - val_loss: 0.3959 - val_accuracy: 0.9074\n",
      "Epoch 161/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.9062e-04 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 0.9074\n",
      "Epoch 162/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.8606e-04 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9074\n",
      "Epoch 163/250\n",
      "124/124 [==============================] - 0s 514us/step - loss: 2.8085e-04 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 0.9074\n",
      "Epoch 164/250\n",
      "124/124 [==============================] - 0s 417us/step - loss: 2.7657e-04 - accuracy: 1.0000 - val_loss: 0.3975 - val_accuracy: 0.9074\n",
      "Epoch 165/250\n",
      "124/124 [==============================] - 0s 433us/step - loss: 2.7338e-04 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 0.9074\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 419us/step - loss: 2.6894e-04 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9074\n",
      "Epoch 167/250\n",
      "124/124 [==============================] - 0s 411us/step - loss: 2.6428e-04 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 0.9074\n",
      "Epoch 168/250\n",
      "124/124 [==============================] - 0s 588us/step - loss: 2.6010e-04 - accuracy: 1.0000 - val_loss: 0.4010 - val_accuracy: 0.9074\n",
      "Epoch 169/250\n",
      "124/124 [==============================] - 0s 564us/step - loss: 2.5640e-04 - accuracy: 1.0000 - val_loss: 0.4026 - val_accuracy: 0.9074\n",
      "Epoch 170/250\n",
      "124/124 [==============================] - 0s 524us/step - loss: 2.5206e-04 - accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.9074\n",
      "Epoch 171/250\n",
      "124/124 [==============================] - 0s 500us/step - loss: 2.4873e-04 - accuracy: 1.0000 - val_loss: 0.4052 - val_accuracy: 0.9074\n",
      "Epoch 172/250\n",
      "124/124 [==============================] - 0s 467us/step - loss: 2.4425e-04 - accuracy: 1.0000 - val_loss: 0.4055 - val_accuracy: 0.9074\n",
      "Epoch 173/250\n",
      "124/124 [==============================] - 0s 476us/step - loss: 2.4240e-04 - accuracy: 1.0000 - val_loss: 0.4055 - val_accuracy: 0.9074\n",
      "Epoch 174/250\n",
      "124/124 [==============================] - 0s 548us/step - loss: 2.3665e-04 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9074\n",
      "Epoch 175/250\n",
      "124/124 [==============================] - 0s 572us/step - loss: 2.3237e-04 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9074\n",
      "Epoch 176/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 2.2956e-04 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9074\n",
      "Epoch 177/250\n",
      "124/124 [==============================] - 0s 492us/step - loss: 2.2597e-04 - accuracy: 1.0000 - val_loss: 0.4093 - val_accuracy: 0.9074\n",
      "Epoch 178/250\n",
      "124/124 [==============================] - 0s 410us/step - loss: 2.2260e-04 - accuracy: 1.0000 - val_loss: 0.4092 - val_accuracy: 0.9074\n",
      "Epoch 179/250\n",
      "124/124 [==============================] - 0s 626us/step - loss: 2.1901e-04 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.9074\n",
      "Epoch 180/250\n",
      "124/124 [==============================] - 0s 476us/step - loss: 2.1583e-04 - accuracy: 1.0000 - val_loss: 0.4116 - val_accuracy: 0.9074\n",
      "Epoch 181/250\n",
      "124/124 [==============================] - 0s 484us/step - loss: 2.1237e-04 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9074\n",
      "Epoch 182/250\n",
      "124/124 [==============================] - 0s 320us/step - loss: 2.0908e-04 - accuracy: 1.0000 - val_loss: 0.4118 - val_accuracy: 0.9074\n",
      "Epoch 183/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.0594e-04 - accuracy: 1.0000 - val_loss: 0.4122 - val_accuracy: 0.9074\n",
      "Epoch 184/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.0278e-04 - accuracy: 1.0000 - val_loss: 0.4122 - val_accuracy: 0.9074\n",
      "Epoch 185/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 2.0068e-04 - accuracy: 1.0000 - val_loss: 0.4109 - val_accuracy: 0.8889\n",
      "Epoch 186/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.9701e-04 - accuracy: 1.0000 - val_loss: 0.4126 - val_accuracy: 0.8889\n",
      "Epoch 187/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.9377e-04 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.8889\n",
      "Epoch 188/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.9021e-04 - accuracy: 1.0000 - val_loss: 0.4154 - val_accuracy: 0.8889\n",
      "Epoch 189/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.8746e-04 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 0.9074\n",
      "Epoch 190/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.8394e-04 - accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 0.9074\n",
      "Epoch 191/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.8106e-04 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 0.9074\n",
      "Epoch 192/250\n",
      "124/124 [==============================] - 0s 441us/step - loss: 1.7862e-04 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9074\n",
      "Epoch 193/250\n",
      "124/124 [==============================] - 0s 395us/step - loss: 1.7632e-04 - accuracy: 1.0000 - val_loss: 0.4188 - val_accuracy: 0.9074\n",
      "Epoch 194/250\n",
      "124/124 [==============================] - 0s 379us/step - loss: 1.7367e-04 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9074\n",
      "Epoch 195/250\n",
      "124/124 [==============================] - 0s 403us/step - loss: 1.7131e-04 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9074\n",
      "Epoch 196/250\n",
      "124/124 [==============================] - 0s 363us/step - loss: 1.6853e-04 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.8889\n",
      "Epoch 197/250\n",
      "124/124 [==============================] - 0s 251us/step - loss: 1.6698e-04 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 0.8889\n",
      "Epoch 198/250\n",
      "124/124 [==============================] - 0s 399us/step - loss: 1.6392e-04 - accuracy: 1.0000 - val_loss: 0.4228 - val_accuracy: 0.8889\n",
      "Epoch 199/250\n",
      "124/124 [==============================] - 0s 274us/step - loss: 1.6139e-04 - accuracy: 1.0000 - val_loss: 0.4230 - val_accuracy: 0.8889\n",
      "Epoch 200/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.5912e-04 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 0.9074\n",
      "Epoch 201/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.5666e-04 - accuracy: 1.0000 - val_loss: 0.4254 - val_accuracy: 0.9074\n",
      "Epoch 202/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.5440e-04 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.9074\n",
      "Epoch 203/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.5196e-04 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.8889\n",
      "Epoch 204/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.4984e-04 - accuracy: 1.0000 - val_loss: 0.4271 - val_accuracy: 0.8889\n",
      "Epoch 205/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.4775e-04 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 0.8889\n",
      "Epoch 206/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.4501e-04 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.8889\n",
      "Epoch 207/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.4293e-04 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.8889\n",
      "Epoch 208/250\n",
      "124/124 [==============================] - 0s 448us/step - loss: 1.4141e-04 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.8889\n",
      "Epoch 209/250\n",
      "124/124 [==============================] - 0s 443us/step - loss: 1.3948e-04 - accuracy: 1.0000 - val_loss: 0.4264 - val_accuracy: 0.8889\n",
      "Epoch 210/250\n",
      "124/124 [==============================] - 0s 451us/step - loss: 1.3769e-04 - accuracy: 1.0000 - val_loss: 0.4259 - val_accuracy: 0.8889\n",
      "Epoch 211/250\n",
      "124/124 [==============================] - 0s 524us/step - loss: 1.3550e-04 - accuracy: 1.0000 - val_loss: 0.4291 - val_accuracy: 0.8889\n",
      "Epoch 212/250\n",
      "124/124 [==============================] - 0s 464us/step - loss: 1.3312e-04 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.8889\n",
      "Epoch 213/250\n",
      "124/124 [==============================] - 0s 756us/step - loss: 1.3119e-04 - accuracy: 1.0000 - val_loss: 0.4327 - val_accuracy: 0.8889\n",
      "Epoch 214/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.2866e-04 - accuracy: 1.0000 - val_loss: 0.4324 - val_accuracy: 0.8889\n",
      "Epoch 215/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.2772e-04 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.8889\n",
      "Epoch 216/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 1.2633e-04 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.8889\n",
      "Epoch 217/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.2352e-04 - accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.8889\n",
      "Epoch 218/250\n",
      "124/124 [==============================] - 0s 463us/step - loss: 1.2172e-04 - accuracy: 1.0000 - val_loss: 0.4367 - val_accuracy: 0.8889\n",
      "Epoch 219/250\n",
      "124/124 [==============================] - 0s 330us/step - loss: 1.2009e-04 - accuracy: 1.0000 - val_loss: 0.4364 - val_accuracy: 0.8889\n",
      "Epoch 220/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.1835e-04 - accuracy: 1.0000 - val_loss: 0.4375 - val_accuracy: 0.8889\n",
      "Epoch 221/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.1659e-04 - accuracy: 1.0000 - val_loss: 0.4377 - val_accuracy: 0.8889\n",
      "Epoch 222/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.1491e-04 - accuracy: 1.0000 - val_loss: 0.4383 - val_accuracy: 0.8889\n",
      "Epoch 223/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.1318e-04 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.8889\n",
      "Epoch 224/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.1165e-04 - accuracy: 1.0000 - val_loss: 0.4391 - val_accuracy: 0.8889\n",
      "Epoch 225/250\n",
      "124/124 [==============================] - 0s 421us/step - loss: 1.1022e-04 - accuracy: 1.0000 - val_loss: 0.4368 - val_accuracy: 0.8889\n",
      "Epoch 226/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0873e-04 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.8889\n",
      "Epoch 227/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0717e-04 - accuracy: 1.0000 - val_loss: 0.4375 - val_accuracy: 0.8889\n",
      "Epoch 228/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0598e-04 - accuracy: 1.0000 - val_loss: 0.4452 - val_accuracy: 0.8889\n",
      "Epoch 229/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0453e-04 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.8889\n",
      "Epoch 230/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 1.0276e-04 - accuracy: 1.0000 - val_loss: 0.4439 - val_accuracy: 0.8889\n",
      "Epoch 231/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 1.0142e-04 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 0.8889\n",
      "Epoch 232/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 9.9902e-05 - accuracy: 1.0000 - val_loss: 0.4442 - val_accuracy: 0.8889\n",
      "Epoch 233/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 9.8648e-05 - accuracy: 1.0000 - val_loss: 0.4448 - val_accuracy: 0.8889\n",
      "Epoch 234/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 9.7395e-05 - accuracy: 1.0000 - val_loss: 0.4466 - val_accuracy: 0.8889\n",
      "Epoch 235/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 9.6265e-05 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8889\n",
      "Epoch 236/250\n",
      "124/124 [==============================] - 0s 435us/step - loss: 9.4562e-05 - accuracy: 1.0000 - val_loss: 0.4457 - val_accuracy: 0.8889\n",
      "Epoch 237/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 9.3528e-05 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.8889\n",
      "Epoch 238/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 9.2471e-05 - accuracy: 1.0000 - val_loss: 0.4464 - val_accuracy: 0.8889\n",
      "Epoch 239/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 9.0750e-05 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.8889\n",
      "Epoch 240/250\n",
      "124/124 [==============================] - 0s 252us/step - loss: 8.9653e-05 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.8889\n",
      "Epoch 241/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 8.8654e-05 - accuracy: 1.0000 - val_loss: 0.4531 - val_accuracy: 0.8889\n",
      "Epoch 242/250\n",
      "124/124 [==============================] - 0s 378us/step - loss: 8.7522e-05 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.8889\n",
      "Epoch 243/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 8.6217e-05 - accuracy: 1.0000 - val_loss: 0.4538 - val_accuracy: 0.8889\n",
      "Epoch 244/250\n",
      "124/124 [==============================] - 0s 455us/step - loss: 8.5082e-05 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.8889\n",
      "Epoch 245/250\n",
      "124/124 [==============================] - 0s 451us/step - loss: 8.3827e-05 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.8889\n",
      "Epoch 246/250\n",
      "124/124 [==============================] - 0s 459us/step - loss: 8.2726e-05 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.8889\n",
      "Epoch 247/250\n",
      "124/124 [==============================] - 0s 508us/step - loss: 8.1657e-05 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.8889\n",
      "Epoch 248/250\n",
      "124/124 [==============================] - 0s 500us/step - loss: 8.0730e-05 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.8889\n",
      "Epoch 249/250\n",
      "124/124 [==============================] - 0s 458us/step - loss: 7.9614e-05 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8889\n",
      "Epoch 250/250\n",
      "124/124 [==============================] - 0s 504us/step - loss: 7.8728e-05 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.8889\n",
      "Test loss: 0.46008338613642585\n",
      "Test accuracy: 0.8888888955116272\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('wine.data',header = None)\n",
    "\n",
    "\n",
    "df.columns = [  'class labels'\n",
    "             \t,'alcohol'\n",
    "             \t,'malicAcid'\n",
    "             \t,'ash'\n",
    "            \t,'ashalcalinity'\n",
    "             \t,'magnesium'\n",
    "            \t,'totalPhenols'\n",
    "             \t,'flavanoids'\n",
    "             \t,'nonFlavanoidPhenols'\n",
    "             \t,'proanthocyanins'\n",
    "            \t,'colorIntensity'\n",
    "             \t,'hue'\n",
    "             \t,'od280_od315'\n",
    "             \t,'proline'\n",
    "                 \n",
    "                ]\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D,Embedding\n",
    "model = Sequential()     \n",
    "model.add(Embedding(1700, 32, input_length=13))\n",
    "model.add(Conv1D(32, 3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adamax',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "training_model=model.fit(X_train, y_train,\n",
    "          batch_size= 15, \n",
    "          epochs=250,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
